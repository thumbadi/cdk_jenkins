import pytest
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType
from pyspark.sql.functions import udf, col
from rawtable import eval_pre_indicator, check_nonnullable_columns, str_to_date_f


@pytest.fixture(scope="module")
def spark():
    spark = SparkSession.builder \
        .appName("pytest") \
        .master("local[*]") \
        .getOrCreate()
    return spark


def test_eval_pre_indicator():
    assert eval_pre_indicator(1, 10) == -10
    assert eval_pre_indicator(0, 10) == 10
    assert eval_pre_indicator(1, 0) == 0


def test_check_nonnullable_columns(spark):
    data = [("A", 1), ("B", 2), ("C", 3)]
    schema = ["col1", "col2"]
    df = spark.createDataFrame(data, schema)
    cols_non_null = ["col1", "col2"]

    # Should not raise any exception
    check_nonnullable_columns(df, cols_non_null, spark)

    # Should raise an exception
    data_with_null = [("A", 1), (None, 2), ("C", 3)]
    df_with_null = spark.createDataFrame(data_with_null, schema)
    with pytest.raises(ValueError):
        check_nonnullable_columns(df_with_null, cols_non_null, spark)


def test_str_to_date_f(spark):
    data = [("20220101",), ("20220102",), ("20220103",)]
    schema = ["date_str"]
    df = spark.createDataFrame(data, schema)

    str_to_date_udf = udf(lambda x: x[:4] + '-' + x[4:6] + '-' + x[6:], StringType())
    formatted_dates_df = df.withColumn("formatted_date", str_to_date_udf(col("date_str")))

    expected_dates = ["2022-01-01", "2022-01-02", "2022-01-03"]
    actual_dates = formatted_dates_df.select("formatted_date").rdd.flatMap(lambda x: x).collect()

    assert actual_dates == expected_dates

# Columns to check for Nullability on Mandatory fields:
def check_nonnullable_columns(df, null_cols, spark):
    for col_name in null_cols:
        has_null = df.filter(col(col_name).isNull()).take(1)
        if has_null:
            raise ValueError(f'Non-nullable Column {col_name} has null value')
        else:
            pass
