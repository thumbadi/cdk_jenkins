from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType,DateType
rdd = spark.read.text("c:/flatfile.txt").rdd

values = [row['value'] for row in rdd.collect()]

iterator = [row['value'] for row in rdd.toLocalIterator()]

schema = [
    {'column': 'record_id', 'type': 'str', 'length': 3},
    {'column': 'sequence_no', 'type': 'int', 'length': 7}, 
    {'column': 'mtf_responsecode', 'type': 'str', 'length': 2},
    {'column': 'adj_deletioncode', 'type': 'str', 'length': 1},
    {'column': 'date_of_service', 'type': 'date', 'length': 8},
    {'column': 'pres_serviceref_no', 'type': 'int', 'length': 12},
    {'column': 'fill_number', 'type': 'int', 'length': 2},
    {'column': 'service_providerId_qual', 'type': 'str', 'length': 2},
    {'column': 'service_provider_id', 'type': 'str', 'length': 15},
    {'column': 'alter_service_provid_qual', 'type': 'str', 'length': 2},
    {'column': 'alter_service_provid_id', 'type': 'str', 'length': 15},
    {'column': 'prescriberId_qual', 'type': 'str', 'length': 2},
    {'column': 'prescriber_id', 'type': 'str', 'length': 35},
    {'column': 'filler', 'type': 'abn', 'length': 40},
    {'column': 'product_service_id', 'type': 'str', 'length': 40},
    {'column': 'qty_dispensed', 'type': 'float', 'length': 10, 'decimal_places': 3},
    {'column': 'days_supply', 'type': 'int', 'length': 3},
    {'column': 'indicator', 'type': 'str', 'length': 1},
    {'column': 'filler', 'type': 'abn', 'length': 40},
    {'column': 'orgi_submit_cont', 'type': 'str', 'length': 5},
    {'column': 'claim_control_no', 'type': 'str', 'length': 40},
    {'column': 'card_holder_id', 'type': 'str', 'length': 20}
]

def split_by_schema(lines, schema):
    """
    Splits the line based on the schema mapping and fills float values accordingly.
    
    :param line: The line to split (string).
    :param schema: A list of dictionaries defining the schema with column length and type.
                   Each schema dict contains:
                   - 'column': Column name (for reference)
                   - 'type': Column type (e.g., 'float', 'int', 'str')
                   - 'length': Total length of the column in the final line
                   - 'decimal_places' (optional for float types): Decimal places for float column type.
    :return: List of split column values.
    """
    values = []
    
    for line in lines:
        start_index = 0
        tmp = []
        print("Testing print line by line reading **************" + str(line))
        for column in schema:
            column_name = column['column']
            column_type = column['type']
            column_length = column['length']
            decimal_places = column.get('decimal_places', 0)
    
            # Get the substring for the current column
            value = line[start_index:start_index + column_length].strip()
    
            if column_type == 'float':
                # For float columns, we need to split the value by the column length
                if value:
                    # If there's no decimal, split the value into integer and decimal portions
                    if '.' not in value:
                        integer_part = value[:column_length - decimal_places]
                        decimal_part = value[column_length - decimal_places:]
                        value = f"{integer_part}.{decimal_part}".strip()
                        # value = float(value)
                    else:
                        # Truncate the value if it exceeds the required decimal length
                        integer_part, decimal_part = value.split('.')
                        decimal_part = decimal_part[:decimal_places]  # Limit decimal part
                        value = f"{integer_part}.{decimal_part}".strip()
                # else:
                #     # If the value is empty, represent it as 0 with the given decimal places
                #     value = '0' * (column_length - decimal_places) + '.' + '0' * decimal_places
    
            elif column_type == 'int':
                # For integer columns, pad with leading zeroes if needed
                value = int(value.zfill(column_length).strip())
    
            elif column_type == 'str':
                # For string columns, pad with spaces to match the column length
                value = value.ljust(column_length).strip()
            elif column_type == 'abn':
                # For string columns, pad with spaces to match the column length
                value = value.ljust(column_length).strip()
            elif column_type == 'date':
                # For string columns, pad with spaces to match the column length
                dt_str = str(value.ljust(column_length).strip())
                value = datetime.datetime.strptime(dt_str,"%Y%m%d").strftime("%Y-%m-%d")
            tmp.append(value)
    
            # Move the start index to the next column's position
            start_index += column_length
        values.append(tuple(tmp))
    return values

import datetime
transformed_data = split_by_schema(values, schema)

columns = StructType([
    StructField("record_id", StringType(), True),
    StructField("sequence_no", IntegerType(), True),
    StructField("mtf_responsecode", StringType(), True),
    StructField("adj_deletioncode", StringType(), True),
    StructField("date_of_service", StringType(), True),
    StructField("pres_serviceref_no", IntegerType(), True),
    StructField("fill_number", IntegerType(), True),
    StructField("service_providerId_qual", StringType(), True),
    StructField("service_provider_id", StringType(), True),
    StructField("alter_service_provid_qual", StringType(), True),
    StructField("alter_service_provid_id", StringType(), True),
    StructField("prescriberId_qual", StringType(), True),
    StructField("prescriber_id", StringType(), True),
    StructField("filler_1", StringType(), True),
    StructField("product_service_id", StringType(), True),
    StructField("qty_dispensed", StringType(), True),
    StructField("days_supply", IntegerType(), True),
    StructField("340b_indicator", StringType(), True),
    StructField("filler_2", StringType(), True),
    StructField("orgi_submit_cont", StringType(), True),
    StructField("claim_control_no", StringType(), True),
    StructField("card_holder_id", StringType(), True)
])


df = spark.createDataFrame(transformed_data, columns)

# until above line from actual script and use one of solution below

solution 1:
pan_df = df.toPandas()
pan_df.to_parquet("C:/test.parquet", engine='pyarrow')

solution 2:
df.write.parquet("C:/test.parquet")
